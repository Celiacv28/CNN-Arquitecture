{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOtf/NG+NKL6BzdWcBFtIof"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from IPython.display import set_matplotlib_formats\n","set_matplotlib_formats('pdf', 'svg')"],"metadata":{"id":"ytp5gE4ycFbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Actividad de Clasificación de Imágenes mediante CNNs\n","\n","En este trabajo se aborda la tarea de clasificación de imágenes utilizando diferentes arquitecturas de Redes Neuronales Convolucionales (CNNs). El objetivo principal es comparar el rendimiento de clasificación obtenido con distintas variantes del modelo.\n","\n","**Metodología**\n","1. Conjunto de datos:\n","  - Se utilizará el conjunto CIFAR10, disponible en la biblioteca PyTorch (torchvision.datasets).\n","  - Se seleccionarán tres clases específicas del conjunto de datos.\n","\n","2. Variantes del modelo;\n","Se implementarán las siguientes versiones de CNN:\n","  - Baseline: 24 canales despúes de la primera capa convolucional.\n","  - Width: 40 características después de la primera capa lineal.\n","  - Regularización: L2 (λ = 0.002).\n","  - Dropout: (probabilidad = 0.6)\n","  - Batch Normalization: 40 características después de la primera capa lineal.\n","  - Depth: 36 canales después de la primera capa convolucional.\n","  - Residual: 36 canales después de la primera capa convolucional.\n","  - Residual Deep: 22 canales después de la primera capa convolucional con 60 bloques.\n","\n","3. Entrenamiento y evaluación\n","  - Se medirá el rendimiento de los clasificadores entrenados.\n","  - Se generarán gráficos comparativos del desempeño en los conjuntos de entrenamiento y validación.\n","\n","4. Tareas adicionales\n","  - Comparación tiempo de entrenamiento para CPU y GPU.\n","  - Evaluación de Robustez ante Ruido Gaussiano.\n","\n","5. Conclusiones\n","  - Se extraerán unas conclusiones del trabajo realizado."],"metadata":{"id":"jWCZCk-sANL6"}},{"cell_type":"code","source":["%matplotlib inline\n","from matplotlib import pyplot as plt\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import collections\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.set_printoptions(edgeitems=2)\n","torch.manual_seed(123)"],"metadata":{"id":"_FDxu81c2eMG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Introducción\n","\n","Una **Red Neuronal Convolucional (CNN)**, es un tipo de algoritmo de **aprendizaje profundo** diseñado principalmente para tareas relacionadas con el reconocimiento de patrones y objetos, como la clasificación, la detección y la segmentación de imágenes.\n","\n","La arquitectura de una CNN se compone de dos partes principales: una **sección convolucional** y una **sección de clasificación**.\n","\n","- La parte convolucional se encarga de extraer las características propias de cada imagen, coprimiendo y reduciendo progresivamente su tamaño mediante capas convolucionales y capas de pooling.\n","- La parte de clasificación consta de capas totalmente conectadas donde se combinan las caracteristicas extraídas por la red para asignar una clase final a la imagen.\n","\n","Las CNN están compuestas por varios tipos de capas:\n","\n","- **Capas Convolucionales**: Extraen características locales (features) aplicando filtros entrenables.\n","- **Funciones de Activación**\n","- **Capas de Pooling**: Reducen la dimensión espacial, mejorando la eficiencia de la red.\n","- **Capas Completamente Conectadas**: Transforman las características extraídas en una predicción final.\n","\n","A medida que los datos avanzan por las capas convolucionales, los mapas de características representan descripciones progresivamente más abstractas y de mayor nivel. En la fase final, las capas totalmente conectadas se encargan de clasificar la imagen completa.\n"],"metadata":{"id":"oMm6NJwUQNYZ"}},{"cell_type":"markdown","source":["## Carga y procesamiento del conjunto de datos\n","\n","El conjunto de datos utilizado es **CIFAR-10**, un benchmark estándar que contiene 60.000 imágenes a color (32x32), distribuidas en 10 clases distintas.\n","\n","Para este trabajo, se han seleccionado específicamente las clases: automóvil, gato y rana.\n","\n","En primer lugar, las imágenes son transformadas utilizando las funciones ToTensor() y Normalize():\n","\n","- ToTensor() convierte las imágenes a tensores de PyTorch.\n","- Normalize() estandariza los valores de los píxeles utilizando la media y la desviación estándar precalculadas de CIFAR-10. Esta normalizacion se aplica por canal (RGB).\n","\n","A continuación, se filtran las clases para obtener un subconjunto de datos con las tres clases seleccionadas.\n","\n","Finalmente se crean los dataloaders.\n","\n","\n"],"metadata":{"id":"sza4ESqA2VAj"}},{"cell_type":"code","source":["class_names = ['airplane','automobile','bird','cat','deer',\n","               'dog','frog','horse','ship','truck']\n","\n","DATA_PATH = '../data-unversioned/p1ch6/'\n","SELECTED_CLASSES = [1, 3, 6]  # automobile, cat, frog\n","CLASS_NAMES = ['automobile', 'cat', 'frog']\n","LABEL_MAP = {1: 0, 3: 1, 6: 2}\n","\n","# Valores de normalización (media y std calculados para CIFAR-10)\n","NORMALIZE_MEAN = (0.4915, 0.4823, 0.4468)\n","NORMALIZE_STD = (0.2470, 0.2435, 0.2616)\n","\n","#Transformaciones\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n","])\n","\n","#Carga de dataset\n","cifar10_train = datasets.CIFAR10(\n","    DATA_PATH,\n","    train=True,\n","    download=True,\n","    transform=train_transform\n",")\n","\n","cifar10_val = datasets.CIFAR10(\n","    DATA_PATH,\n","    train=False,\n","    download=True,\n","    transform=val_transform\n",")\n","\n","\n","#Filtrado\n","\n","def filter_classes(dataset, selected_classes, label_map):\n","    \"\"\"Filtra dataset manteniendo solo las clases seleccionadas\"\"\"\n","    data = [(img, label_map[label])\n","            for img, label in dataset\n","            if label in selected_classes]\n","    return data\n","\n","cifar3_train = filter_classes(cifar10_train, SELECTED_CLASSES, LABEL_MAP)\n","cifar3_val = filter_classes(cifar10_val, SELECTED_CLASSES, LABEL_MAP)\n","\n","\n","train_loader = DataLoader(cifar3_train, batch_size=32, shuffle=True)\n","val_loader = DataLoader(cifar3_val, batch_size=32, shuffle=False)\n","\n","#Verificación de datos\n","sample_img, sample_label = cifar3_train[0]\n","print(f\"\\nInformación del dataset:\")\n","print(f\"- Número de muestras entrenamiento: {len(cifar3_train)}\")\n","print(f\"- Número de muestras validación: {len(cifar3_val)}\")\n","print(f\"- Shape de las imágenes: {sample_img.shape}\")"],"metadata":{"id":"5uixE7c-2f8a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## 3. Definición de modelos\n","\n","### Modelo Baseline\n","\n","El modelo **Baseline** implementa una Red Neuronal Convolucional secuencial, utilizando filtros bidimensionales de tamaño 3x3.  Su arquitectura combina capas convolucionales, de activación, pooling y una capa completamente conectada para clasificación.\n","\n","Como functión de activación se utiliza ReLU (Función lineal rectificada). Esta función introduce no linealidad, permitiendo el aprendizaje de patrones complejos en grandes volúmenes de datos. Mitiga el problema de la desaparición de gradientes (a diferencia de las funciones sigmoide y tanh).\n","\n","**Arquitectura detallada**\n","\n","- Capa convulacional Conv2d(3, 24, kernel_size=3, padding=1):\n","  - Entrada: imagen con tres canales (RGB), tamaño 32x32.\n","  - Parámetros:\n","    - 24 filtros de 3x3.\n","    - Padding = 1: Mantiene el tamaño de la imagen.\n","  - Salida:  Tensor de [24, 32, 32].\n","\n","- ReLu() + MaxPool2d(2):\n","  - ReLU(): Activa solo los valores positivos e introduce no linealidad.\n","  - MaxPool2d(2): Reduce dimensiones a la mitad → [24, 16, 16].\n","\n","- Capa convulucional Conv2d(24, 48, kenerl_size=3, padding=1):\n","  - Entrada: 24 canales, tamaño 16x16-\n","  - Parámetros:\n","    - 48 filtos de 3x3.\n","    - Padding = 1.\n","  - Salida: Tensor de [48, 16, 16].\n","\n","- ReLU() + MaxPool2d(2):\n","  - Similar al paso anterior. Salida: [48, 8, 8].\n","\n","- Flatten():\n","  - Convierte el tensor 3D en un vector 1D.\n","  - Entrada: [batch_size, 48, 8, 8]\n","  - Salida: [batch_size, 3072] (48x8x8)\n","\n","- Capa completamente conectada:\n","  - Linear(3072, 128): Conecta las 3072 entradas a 128 neuronas.\n","  - ReLU: Introduce no linealidad\n","  - Linear(128, 3): Capa final para clasificación en 3 clases (coche, gato, rana)."],"metadata":{"id":"QzmszBoQlhU3"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Conv2d(3, 24, kernel_size=3, padding=1),  # 1ª conv\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),                             # Reduce a la mitad --> [24, 16, 16]\n","\n","    nn.Conv2d(24, 48, kernel_size=3, padding=1), # 2ª conv\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),                             # Reduce a la mitad --> [48, 8, 8]\n","\n","    nn.Flatten(),\n","    nn.Linear(48*8*8, 128),                      # capa oculta\n","    nn.ReLU(),\n","    nn.Linear(128, 3)                            # salida para 3 clases\n",")\n","\n","numel_list = [p.numel() for p in model.parameters()]\n","sum(numel_list), numel_list"],"metadata":{"id":"Jne15oTA1kep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_img, _ = cifar3_train[0]  # Una imagen del conjunto filtrado\n","sample_img = sample_img.unsqueeze(0)  # Añade dimensión batch: [1, 3, 32, 32]\n","\n","# Imagen original\n","plt.imshow(np.transpose(sample_img.squeeze().numpy(), (1, 2, 0)))\n","plt.title(\"Imagen original (entrada)\")\n","plt.axis('off')\n","plt.show()\n","\n","# Pasa la imagen por las primeras capas (hasta el segundo MaxPool)\n","with torch.no_grad():\n","    conv1 = model[0](sample_img)   # Conv2d(3→24)\n","    relu1 = model[1](conv1)\n","    pool1 = model[2](relu1)\n","\n","    conv2 = model[3](pool1)        # Conv2d(24→48)\n","    relu2 = model[4](conv2)\n","    pool2 = model[5](relu2)\n","\n","# Mapas de activación tras la segunda capa convolucional\n","def show_feature_maps(tensor, title):\n","    fig, axs = plt.subplots(3, 6, figsize=(12, 6))\n","    for i in range(18):\n","        axs[i // 6, i % 6].imshow(tensor[0, i].numpy(), cmap='gray')\n","        axs[i // 6, i % 6].axis('off')\n","    plt.suptitle(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n","show_feature_maps(conv1, \"Salida Conv1 (24 canales)\")\n","show_feature_maps(pool1, \"Tras MaxPool1\")\n","show_feature_maps(conv2, \"Salida Conv2 (48 canales)\")\n","show_feature_maps(pool2, \"Tras MaxPool2\")\n"],"metadata":{"id":"iaktsBbyp4wp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo width\n","El modelo **Width** tiene 40 neuronas despúes de la primera capa lineal,  en lugar de las 128 neuronas que se usan en el modelo base.\n","\n","Esto reduce la dimensión del espacio latente aprendido antes de la salida."],"metadata":{"id":"43LLHAX5DFmu"}},{"cell_type":"code","source":["model_width = nn.Sequential(\n","    nn.Conv2d(3, 24, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Conv2d(24, 48, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(48),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Flatten(),\n","    nn.Linear(48 * 8 * 8, 40),       # ← 40 features\n","    nn.ReLU(),\n","    nn.Linear(40, 3)\n",")\n"],"metadata":{"id":"4xMckF8MDk6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo Batch Normalization\n","\n","El modelo **Batch Normalization** añade normalización por lotes después de cada capa convolucional."],"metadata":{"id":"Q98eRiY2qSdC"}},{"cell_type":"code","source":["model_bn = nn.Sequential(\n","    nn.Conv2d(3, 24, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(24),              # ← se normaliza cada canal\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Conv2d(24, 48, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(48),              # ← también en la segunda conv\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Flatten(),\n","    nn.Linear(48*8*8, 40),\n","    nn.ReLU(),\n","    nn.Linear(40, 3)\n",")"],"metadata":{"id":"ButUydRoqgBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo con Dropout\n","\n","El modelo **Dropout** incorpora la técnica de regularización dropout para mitigar el sobreajuste (overfitting) durante el entrenamiento. Dropout desactiva aleatoriamente un porcentaje de neuronas durante el entrenamiento (en este caso el 60%) en la capa designada durante cada iteración de entrenamiento. Esto reduce la dependencia del modelo hacia neuronas específicas, promoviendo así una mejor generalización.\n","\n","La arquitectura es similar al modelo baseline, unicamente añadiendo una capa Dropout(p=0.6) después de la primera capa lineal."],"metadata":{"id":"0eGHGar-H5Jk"}},{"cell_type":"code","source":["model_dropout = nn.Sequential(\n","    nn.Conv2d(3, 24, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Conv2d(24, 48, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","\n","    nn.Flatten(),\n","    nn.Linear(48 * 8 * 8, 128),\n","    nn.ReLU(),\n","    nn.Dropout(p=0.6),          # ← Dropout\n","    nn.Linear(128, 3)\n",")\n"],"metadata":{"id":"iiOx1mAzELST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo con mayor profondidad y residual\n","\n","En el modelo **Depth** se aumenta la profundidad de la red convolucional, es decir, se añaden más capas convolucionales permitiendo que la red aprenda características más complejas de los datos.\n","\n","Los cambios de esta versión incluyen, obtener 36 canales despúes de su primera capa convolucional y añadir una capa convulacional adicional con un mayor número de filtros."],"metadata":{"id":"B5s5JUSVIVAZ"}},{"cell_type":"code","source":["model_depth = nn.Sequential(\n","    nn.Conv2d(3, 36, kernel_size=3, padding=1),   # ← 36 canales\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),                              # 32 → 16\n","\n","    nn.Conv2d(36, 72, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),                              # 16 → 8\n","\n","    nn.Conv2d(72, 128, kernel_size=3, padding=1),  # capa extra\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),                              # 8 → 4\n","\n","    nn.Flatten(),\n","    nn.Linear(128 * 4 * 4, 128),\n","    nn.ReLU(),\n","    nn.Linear(128, 3)\n",")\n"],"metadata":{"id":"ONtITp8TEZJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo residual\n","\n","El modelo **Residual** implementa bloques residuales. Los bloques residuales fueron introducidos en ResNet, permitiendo conexiones de saltos (skip connections) que permiten el flujo directo del gradiente, mitigando el problema del *vanishing gradient* en redes profundas.\n","\n","En redes tradicionales, cuando el gradiente se retropropaga, puede volverse extremadamente pequeño debido a la repetida multiplicación en capas sucesivas. Las conexiones residuales simplifican este proceso al sumar la entrada original al resultado de las convoluciones, usando menos capas durante la etapa de entrenamiento inicial. Esto permite que la red aprenda ajustes (residuos) en lugar de construir toda la información desde cero.\n","\n","En el bloque residual, se aplica dos capas convolucionales (cv1 y cv2) al mismo tensor de entrada. Luego, la salida de la segunda convolución se suma a la entrada original, creando la conexión residual.\n","\n","En este modelo se utilizan dos bloques residuales con 36 canales cada uno. Cada bloque realiza dos convoluciones seguidas de la conexión residual."],"metadata":{"id":"oLzUET7XF1dc"}},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        return self.relu(self.conv2(self.relu(self.conv1(x))) + x)\n","\n","class ResidualCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.start = nn.Sequential(\n","            nn.Conv2d(3, 36, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.res1 = ResidualBlock(36)\n","        self.res2 = ResidualBlock(36)\n","        self.end = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            nn.Flatten(),\n","            nn.Linear(36*8*8, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 3)\n","        )\n","\n","    def forward(self, x):\n","        x = self.start(x)\n","        x = self.res1(x)\n","        x = self.res2(x)\n","        return self.end(x)"],"metadata":{"id":"22HOOlMGF30q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modelo residual profundo\n","\n","El modelo **Residual Deep** tiene 22 canales despues de su primera capa convolucional y usa 60 bloques residuales consecutivos."],"metadata":{"id":"8Pt-OzOKI1vv"}},{"cell_type":"code","source":["class ResidualDeepCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.start = nn.Sequential(\n","            nn.Conv2d(3, 22, kernel_size=3, padding=1), #22 Canales después de su primera capa convolucional\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.res_blocks = nn.Sequential(*[ResidualBlock(22) for _ in range(60)]) #60 Bloques residuales\n","        self.end = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            nn.Flatten(),\n","            nn.Linear(22*8*8, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 3)\n","        )\n","\n","    def forward(self, x):\n","        x = self.start(x)\n","        x = self.res_blocks(x)\n","        return self.end(x)"],"metadata":{"id":"6IikAG1bGRoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento del modelo\n","\n","En este apartado se implementa el proceso completo de entrenamiento de la red neuronal. Cada modelo se entrena durante 10 ciclos completos de entrenamiento. La función train_model se encarga de:\n","\n","- **Configuración inicial**: Detecta automáticamente si hay GPU disponible (CUDA). En caso contrario utiliza CPU.\n","- **Función de pérdida - CrossEntropyLoss()**: Evalúa el rendimiento comparando las predicciones resultantes con las etiquetas reales.\n","- **Optimizador SGD**: Actualiza los pesos del modelo en función del gradiente calculado. Aplica regularización L2.\n","- **Ciclo de entrenamiento**: Por cada época (iteración completa sobre los datos):\n","  - Fase de entrenamiento: Procesa los datos por lotes (batches) y calcula predicciónes y pérdidas.\n","  - Fase de validación: Evalúa el modelo y calcula métricas de precisión."],"metadata":{"id":"f0Gcvh26oDS-"}},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, l2_lambda=0.0):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=l2_lambda)\n","\n","    train_acc_list = []\n","    val_acc_list = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        correct = 0\n","        total = 0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()           #Limpia los gradientes\n","            outputs = model(images)         #Forward: Predicción\n","            loss = loss_fn(outputs, labels) #Calcula pérdida\n","            loss.backward()                 #Calcula gradientes\n","            optimizer.step()                #Actualiza pesos\n","\n","            preds = outputs.argmax(1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_acc = correct / total\n","        train_acc_list.append(train_acc)\n","\n","        # Evaluación\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                preds = outputs.argmax(1)\n","                correct += (preds == labels).sum().item()\n","                total += labels.size(0)\n","        val_acc = correct / total\n","        val_acc_list.append(val_acc)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n","\n","    return train_acc_list, val_acc_list\n"],"metadata":{"id":"SUbMIwahobB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {}\n","\n","# Modelo baseline\n","print(\"Baseline:\")\n","results[\"Baseline\"] = train_model(model, train_loader, val_loader,)\n","\n","# Modelo width\n","print(\"Width:\")\n","results[\"Width\"] = train_model(model_width, train_loader, val_loader)\n","\n","#Modelo Batch Normalization\n","print(\"BN:\")\n","results[\"BN\"] = train_model(model_bn, train_loader, val_loader)\n","\n","# Modelo Dropout\n","print(\"Dropout:\")\n","results[\"Dropout\"] = train_model(model_dropout, train_loader, val_loader)\n","\n","# Modelo Depth\n","print(\"Depth:\")\n","results[\"Depth\"] = train_model(model_depth, train_loader, val_loader)\n","\n","# Modelo Residual\n","print(\"Residual:\")\n","residual_model = ResidualCNN()\n","results[\"Residual\"] = train_model(residual_model, train_loader, val_loader)\n","\n","# Modelo Residual Deep\n","print(\"Residual Deep:\")\n","residual_deep_model = ResidualDeepCNN()\n","results[\"Residual_Deep\"] = train_model(residual_deep_model, train_loader, val_loader, num_epochs=6)\n","\n","# Modelo con L2 regularization (mismo que baseline pero con lambda)\n","print(\"L2 Regularization:\")\n","model_l2 = nn.Sequential(\n","    nn.Conv2d(3, 24, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Conv2d(24, 48, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Flatten(),\n","    nn.Linear(48 * 8 * 8, 128),\n","    nn.ReLU(),\n","    nn.Linear(128, 3)\n",")\n","results[\"L2 Regularization\"] = train_model(model_l2, train_loader, val_loader, l2_lambda=0.002)\n"],"metadata":{"id":"sAbjwTcbofbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 6))\n","\n","for name, (train_acc, val_acc) in results.items():\n","    plt.plot(train_acc, linestyle='--', label=f\"{name} (Train)\")\n","\n","for name, (train_acc, val_acc) in results.items():\n","    plt.plot(val_acc, linestyle='-', label=f\"{name} (Val)\")\n","\n","plt.title(\"Precisión de Entrenamiento y Validación por Modelo\")\n","plt.xlabel(\"Época\")\n","plt.ylabel(\"Precisión\")\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"f1cJSBmzPh_t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A continuación, se presenta un análisis comparativo del rendimiento de los modelos:\n","\n","**Modelos con mejor desempeño: Width y Batch Normalization**\n","\n","- Width: Precisión de validación 0.87, con una curva de aprendizaje estable y sin señales de sobreajuste. Este modelo supera al baseline desde las primeras iteraciones gracias a un espacio latente más compacto (40 neuronas en lugar de 128), lo que mejora su capacidad de generalización.\n","- Batch Normalization: Muestra el mejor desempeño, alcanzando una validación final de 0.88. Estabiliza el entrenamiento al normalizar las activaciones.\n","\n","**Modelo Baseline (Referencia)**\n","- El modelo Baseline alcanza 0.79 en la última iteración. Su rendimiento es consistente pero inferior al de Width y BN.\n","\n","**Regularización (Dropout y L2)**\n","\n","- Dropout: Mejora de 0.53 a 0.76. Muestra un aprendizaje lento, ya que desactiva un 60% de las neuronas durante el entrenamiento, lo que limita su capacidad máxima, aunque mejora la generalización.\n","- L2 Regularization: Muestra una mejora constante (0.53 → 0.77). Penaliza pesos grandes, evitando sobreajuste sin sacrificar tanto rendimiento como Dropout.\n","\n","**Modelos Profundos: Depth y Residual**\n","\n","- Depth: Comienza con 0.33 de validación y llega a 0.71, mostrando underfitting al inicio y un aprendizaje más lento.\n","- Residual: Llega hasta 0.83 en validación, mostrando que las conexiones de salto mejoran el flujo de gradiente y aceleran el aprendizaje en comparación con Depth.\n","\n","**Residual Deep**\n","\n","- La precisión final después de 6 epocas es 0.80. Esto muestra un buen rendimiento en menos épocas, pero con ligera inestabilidad al final.\n","\n","**Conclusiones finales**\n","\n","En conclusión, los modelos Width y Batch Normalization muestran los mejores rendimientos, con BN mostrando un equilibrio ligeramente mejor entre ajuste y robustez.\n","\n","Estos resultados pueden ser debidos a:\n","1. El tamaño reducido del dataset (solo 3 clases de CIFAR-10): Limitó la efectividad de modelos complejos (Depth, Residual Deep), que requieren más datos para evitar sobreajuste.\n","2. Capacidad del modelo y regularización: Arquitecturas más anchas (Width) o con normalización (BN) lograron un mejor equilibrio entre aprendizaje y generalización.\n","3. Sencillez vs. complejidad: Modelos como Width y BN priorizaron la extracción de características esenciales sin memorizar ruido, a diferencia de alternativas más complejas que tendieron a sobreajustarse.\n","\n","En resumen, la simplicidad controlada (Width) y la normalización (BN) son estrategias ganadoras cuando los datos son limitados.\n","\n"],"metadata":{"id":"gNavvbeqooYz"}},{"cell_type":"markdown","source":["## Tareas adicionales\n","\n","### Comparación tiempo de entrenamiento para CPU y GPU\n","\n","En esta sección se ha realizado una comparación entre el tiempo de entramiento en CPU y en GPU, para los modelos baseline, width y bn."],"metadata":{"id":"-sOGxRhNsGvY"}},{"cell_type":"code","source":["def train_with_timing(model, train_loader, val_loader, device_str='cpu', epochs=5, lr=0.001, l2_lambda=0.0):\n","    device = torch.device(device_str)\n","    model = model.to(device)\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_lambda)\n","\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                _ = model(images)\n","\n","    total_time = time.time() - start_time\n","    return total_time\n"],"metadata":{"id":"lr-f6wXGsPiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["times = {}\n","\n","models = {\n","    \"Baseline\": model,\n","    \"Width\": model_width,\n","    \"BN\": model_bn\n","}\n","\n","# Comparación CPU vs GPU\n","for name, mdl in models.items():\n","    print(name)\n","    cpu_time = train_with_timing(mdl, train_loader, val_loader, device_str='cpu')\n","    print(f\"CPU time: {cpu_time}\")\n","\n","    mdl.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n","\n","    gpu_time = train_with_timing(mdl, train_loader, val_loader, device_str='cuda')\n","    print(f\"GPU time: {gpu_time}\")\n","\n","    times[name] = {\"CPU\": cpu_time, \"GPU\": gpu_time}"],"metadata":{"id":"CgyyKtOfsTfa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"282deca3-780e-448e-f5a0-3c7b1849185b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline\n","CPU time: 75.59359097480774\n","GPU time: 6.359274864196777\n","Width\n","CPU time: 70.76189064979553\n","GPU time: 6.162753105163574\n","BN\n"]}]},{"cell_type":"code","source":["labels = list(times.keys())\n","cpu_times = [times[m][\"CPU\"] for m in labels]\n","gpu_times = [times[m][\"GPU\"] for m in labels]\n","\n","x = np.arange(len(labels))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(10,6))\n","bars1 = ax.bar(x - width/2, cpu_times, width, label='CPU')\n","bars2 = ax.bar(x + width/2, gpu_times, width, label='GPU')\n","\n","ax.set_ylabel('Tiempo (segundos)')\n","ax.set_title('Comparación de tiempos de entrenamiento (CPU vs GPU)')\n","ax.set_xticks(x)\n","ax.set_xticklabels(labels, rotation=45)\n","ax.legend()\n","ax.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"tWrwfPwytf28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como se puede observar en la gráfica anterior, el tiempo de entrenamiento en GPU es significativamente menor que el tiempo en CPU.\n","- **Entrenamiento CPU**: Los tiempos se sitúan entre 77 y 83 segundos por modelo. El proceso en CPU es mucho más lento, debido a la naturaleza secuencial y limitada de las operaciones en CPU.\n","- **Entrenamiento GPU**: Los tiempos se reducen drásticamente a 6-7 segundos, lo que supone una aceleración de más de 10 veces en comparación con la CPU."],"metadata":{"id":"bugXLKws3Eeh"}},{"cell_type":"markdown","source":["### Evaluación de robustez ante Ruido Gaussiano\n","\n","Con el objetivo de evaluar la capacidad de los modelos frente al ruido, se ha evaluado el rendimiento en el conjunto de validación añadiendo **ruido gaussiano** a las imágenes.\n","\n","El ruido se genera con **torch.randn**, que produce valores con distribución normal (media 0, desviación estándar 1), y se escala por un factor σ."],"metadata":{"id":"AT0k-wtk_uhr"}},{"cell_type":"code","source":["def evaluate_with_noise(model, val_loader, noise_levels, device=\"cuda\"):\n","    model.eval()\n","    model.to(device)\n","    accuracies = []\n","\n","    with torch.no_grad():\n","        for sigma in noise_levels:\n","            correct = 0\n","            total = 0\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","\n","                # Añadir ruido gaussiano\n","                noisy_images = images + sigma * torch.randn_like(images)\n","                noisy_images = torch.clamp(noisy_images, 0, 1)\n","\n","                outputs = model(noisy_images)\n","                preds = outputs.argmax(1)\n","                correct += (preds == labels).sum().item()\n","                total += labels.size(0)\n","\n","            acc = correct / total\n","            accuracies.append(acc)\n","            print(f\"Ruido sigma={sigma:.2f} -> Precisión: {acc:.4f}\")\n","\n","    return accuracies\n","\n","\n","\n","noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","models = {\n","    \"Width\": model_width,\n","    \"Dropout\": model_dropout,\n","    \"Residual\": residual_model\n","}\n","\n","results_noise={}\n","\n","for name, mdl in models.items():\n","  print(f\"Modelo {name}\")\n","  acc_noise = evaluate_with_noise(mdl, val_loader, noise_levels)\n","  results_noise[name] = acc_noise\n","\n","plt.figure(figsize=(8, 5))\n","for name, acc_noise in results_noise.items():\n","    plt.plot(noise_levels, acc_noise, marker='o', label=name)\n","\n","plt.title(\"Comparación de Robustez ante Ruido Gaussiano\")\n","plt.xlabel(\"Desviación estándar del ruido (σ)\")\n","plt.ylabel(\"Precisión en Validación\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"ZLL1AB7EAANL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Los resultados muestran como la precisión de los modelos Width, Dropout y Residual se ve afectada al introducir ruido gaussiano en el conjunto de validación.\n","\n","- **Modelo Width**: La precisión empieza en 0.46 con datos sin ruido, y alcanza 0.56 en 0.3, para luego descender. Este comportamiento sugiere que pequeñas cantidades de ruido pueden actuar como regularización, mejorando ligeramente la capacidad del modelo para generalizar. Si el ruido es ecesivo, la precisión disminuye.\n","- **Modelo Dropout**: La precisión es muy estable en todo el rango de ruido (0.47-0.48), mostrando casi ninguna variación. Esto ocurre porque Dropout ya entrena con \"ruido interno\" (desactivación aleatoria de neuronas), por lo que el modelo es más robusto a perturbaciones externas.\n","- **Modelo Residual**: La precisión comienza en 0.45 y desciende progresivamente hasta 0.39 al aumentar el ruido. El modelo Residual parece más sensible al ruido, probablemente porque las conexiones residuales amplifican ciertas características de la imagen que se ven degradadas cuando se introducen perturbaciones."],"metadata":{"id":"qVr9tg0n5P1l"}},{"cell_type":"markdown","source":["## Conclusión\n","\n","Tras definir y entrenar distintas arquitecturas de CNN utilizando el conjunto de datos CIFAR-10 (centrándonos en tres clases específicas), hemos obtenido las siguientes conclusiones:\n","\n","- Los modelos más efectivos son Batch Normalization y Width. BN alcanzó la mejor precisión en validación (~0.88), beneficiándose de la normalización de activaciones que estabiliza y acelera el entrenamiento.\n","- Dropout (p=0.6) y L2 Regularization ayudaron a reducir el sobreajuste, aunque con una ligera pérdida en la precisión máxima.\n","- Modelos profundos (Depth y Residual Deep) mostraron un aprendizaje más lento y una mayor necesidad de datos o de más épocas para alcanzar su máximo potencial.\n","- Residual, con conexiones de salto, ofreció una mejora notable respecto a Depth (0.83 vs. 0.71 en validación), confirmando la importancia de las conexiones residuales para el flujo de gradiente.\n","\n","En el análisis de robustez con ruido gaussiano, Dropout resultó ser el modelo más estable (manteniendo ~0.47 de precisión en todo el rango de ruido), mientras que Width mostró un interesante pico de rendimiento con ruido moderado, lo que indica una buena capacidad de generalización.\n","\n","El trabajo confirma que modelos más simples pero bien regularizados (Width o BN) pueden superar en rendimiento y estabilidad a arquitecturas más profundas, especialmente cuando el conjunto de datos es reducido."],"metadata":{"id":"vcHQPPQoAn4f"}},{"cell_type":"code","source":["!jupyter nbconvert --to PDF \"/content/drive/MyDrive/Colab Notebooks/ObjectClassificationActivity.ipynb\""],"metadata":{"id":"GhQHeiZ8AmJw"},"execution_count":null,"outputs":[]}]}